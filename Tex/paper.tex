%\documentclass{article}
\documentclass[twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=0.6in}


\title{IFT6390 Project1}
\author{Marthy Garcia, Rafael Hernandez}
\date{March 2021}

\begin{document}

\maketitle

\section{Introduction}
This paper report on the design and documentation of a machine learning model, as a part of a Introduction to Machine learning course at the University of Montreal. 

Our main task is to predict the mean per capita (100,000) cancer mortalities for a given set of details about US counties. In this document, the authors describe the pre-processing of data, model selection, testing, hyper-parameter tuning, and validation.

\section{Data } \label{secData}

The data used in this project can be found in \cite{bibdata}. The data has various attributes about the US counties, cancer rates, resident characteristics distribution.The train data-set comprises 32 features and 3047 observations containing numerical and categorical variables. The full data dictionary can be found in \cite{bibdatadict}.

Features with more than 50\% missing values were dropped (e.g."pctsomecol18\_24"). On numerical variables missing values were replaced by the mean of the fields. Categorical variables did not have missing values.

\subsection{Training and test data sets} \label{secsplit}
The original data was split in training and testing data set. As advised in \cite{bibDangeti2017},  20\% of the data was randomly selected for testing, the remaining data was assigned to training.

\section{Pre-processing steps} \label{secpre}
An exploratory data analysis (EDA) that comprised correlation and distribution analysis was performed on the training data set to inform model selection. Figure \ref{fig:corr} depicts the correlation matrix. Multiple variables show a strong correlation within them. As an example, the mean per capita cancer mortalities has a positive relationship with the median income per county. As per Figure \ref{fig:corr}, the median income per county is highly correlated with multiple variables as well. This indicates that we should perform feature selection before implementing a model.

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{correlation.png}
    \caption{Correlation matrix}
    \label{fig:corr}
\end{figure}

Figure \ref{fig:linear} shows the target variable against the feature: \% of country residents with public coverage.  The latter depicts a linear relationship. The same behaviour is observed with multiple features. Therefore a linear regression seems promising.

\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{linearrelation.png}
    \caption{Target variable vs \% of country residents with public coverage}
    \label{fig:linear}
\end{figure}

Most of the numerical variables follow skewed distributions and have different scales. Therefore, a logarithmic transformation was applied, ultimately improving our model fit and score. As a result, a logarithmic transformation increased the accuracy of the model by almost 60\%. As for categorical variables, they were transformed using one-hot encoding. 

\section{Model Selection}\label{secmodel}

\subsection{Hyper-parameter tuning} \label{secpar}
As per Figure \ref{fig:corr}, some features exhibit a high correlation. Feature selection was done by applying a grid search Recursive Feature Elimination (RFE) cross-validation. To verify the validity of the model, a 10- fold cross-validation was selected and later compared based on their respective $R^2$ performance. By looking at the figure \ref{fig:rfe}, once again, using the $R^2$ measure, we can compare the performance obtained by both the training set and the validation set during the feature selection method. Visually we can notice the slope becoming horizontal around 15 features, making it a sound number for choosing our features.


\begin{figure}[h]
    \centering
    \includegraphics[width=7cm]{RFE.png}
    \caption{$R^2$ by number of features}
    \label{fig:rfe}
\end{figure}

\subsection{Linear Regression}
 Linear regression is a method that finds the best linear fit to a given independent and dependent variables. The best fit is found by minimizing the sum of squared residuals.
 
 As per the findings from Figure \ref{fig:linear}, a linear regression method was chosen as per its simplicity, interpretability, and simple implementation. The model is given as follow:
 
 $$\mathbf{y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon}$$
 Where
 
 \[ \mathbf{X}=\left[ \begin{array}{cccc}
1 & x_{1,1}  &...&x_{1,15}\\
1 & x_{2,1}  &...&x_{3,15}\\
\vdots & \vdots &...&\vdots\\
1 & x_{n,1} &...&x_{n,15}
\end{array} \right]
%
\mathbf{y}=\left[ \begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{array} \right]
%
\mathbf{\epsilon}=\left[ \begin{array}{c}
e_{1} \\
e_{2} \\
\vdots \\
e_{n}
\end{array} \right]
\]
The vector $\mathbf{\beta}$ consists of the weight of each feature. We solve for $\mathbf{\beta}$ as follows:
$$\mathbf{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$ \label{eqA}
\subsection{Time and space complexity}
As per \cite{{bibBanerjee20}} the training complexity of the model is $O(k^2(n+k))$. Where $k$ corresponds to the number of features.
The test runtime complexity of linear regression is $O(k)$.

The training space complexity is $O(nk+n)$. The test space complexity at run time is $O(k)$.

\section{Model score}\label{secscore}
To ensure reproductivity of the results, a seed was set in all steps. The model was run in Colab free version. Due to the size of the dataset being small, the average fit time was 0.106s with a standard deviation of 0.019.

Performance was validated by applying a 10- fold cross-validation and measured by the $R^2$. Figure~\ref{fig:rsqrd_trainval} shows the $R^2$ value for the training and validation sets. The test $R^2$ obtained is 0.775. The Mean Absolute Error (MAE) and the Mean Squared Error (MSE) were also collected. Table~\ref{tabresults} summarized the results for the training, validation and test data sets.

\begin{center}\label{tabresults}
\begin{tabular}{ |c|c|c|c|c| } 
 \hline
 Set   &mean $R^2$ &std.dev $R^2$   & MAE   & MSE \\
 \hline
 Train      &0.787      & 0.041                     &0.053  &0.005 \\ 
 Valid &0.778      &0.041                      &0.054  & 0.054\\ 
 Test       & 0.775     &-                          &0.054  & 0.072\\ 
 \hline
\end{tabular}
\end{center}

\begin{figure}
    \centering
    \includegraphics[width=7cm]{r_sqdtrainvalid.png}
    \caption{Train and validation $R^2$ per fold}
    \label{fig:rsqrd_trainval}
\end{figure}

\section{Conclusion and further work}\label{secconclusion}

The objective of our model was to predict the cancer mortality mean per capita (100,000). We performed the following steps as an iterative process: feature engineering (pre-processing), feature selection and model selection. 

Logarithmic transformation of numerical variables significantly increase the performance of our model. An initial EDA, showed a high correlation between the variables. Hence, RFE was applied to select the most important features. A linear regression was built. Cross-validation helped to create a more robust model by verifying the distribution of our performance. For instance, we can see on ~\ref{fig:rsqrd_trainval} that our model did not perform well on the last fold. 

This project had limitation in the gathering of additional information. We recommend that future versions of the model are focused on collecting additional observations.


%%%%%%%%%%% References%%%%%%%%%%%%%%%%%%%%%%%%%%
{\footnotesize
\begin{thebibliography}{100} % 100 is a random guess of the total number of %references

\bibitem{bibdata}
Linear Regression Exercise 1: Data word \texttt{https://data.world/exercises/linear-regression-exercise-}
\texttt{1/workspace/project-summary?agentid=exercises&datasetid=}
\texttt{linear-regression-exercise-1}

\bibitem{bibdatadict} 
Linear Regression Exercise 1- metadata: Data word,\texttt{https://data.world/exercises/linear}
\texttt{-regression-exercise-1/workspace/data-dictionary}

\bibitem{bibDangeti2017} Dangeti, Pratap, ``Statistics for Machine Learning: Techniques for Exploring Supervised, Unsupervised, and Reinforcement Learning Models with Python and R," 2017, \emph{Packt Publishing}

\bibitem{bibBanerjee20}
Banerjee, Writuparna,``Train/Test Complexity and Space Complexity of Linear Regression," 2020,\texttt{https://levelup.gitconnected.com/}

\end{thebibliography}
}
%%%%%%%%%%%%% end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
